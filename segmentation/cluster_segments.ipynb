{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBRVE9F5yvzB/EfaMM4cQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joris-vaneyghen/mss-jazz-playalong/blob/main/segmentation/cluster_segments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhDdvn1sr6nF"
      },
      "outputs": [],
      "source": [
        "# download our audio example\n",
        "!git clone https://github.com/joris-vaneyghen/mss-jazz-playalong.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "zNHLwMflsUEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mss-jazz-playalong/sound_classes_audioset.json', 'r') as f:\n",
        "  sound_classes_audioset = json.load(f)"
      ],
      "metadata": {
        "id": "gNcEaWGuGJ6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = 'mss-jazz-playalong/examples'\n",
        "output_path = 'mss-jazz-playalong/out/segment_and_tag'\n",
        "resolution = 0.32 # resolution of EfficientAT model"
      ],
      "metadata": {
        "id": "afJdx6ogsZcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(dir, mp3_file):\n",
        "    # Replace .mp3 extension with .json\n",
        "    json_file_name = mp3_file.replace('.mp3', '.json')\n",
        "    file_path = os.path.join(dir, json_file_name)\n",
        "\n",
        "    # Check if the .json file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        return {}  # Return an empty dictionary if the .json file doesn't exist\n",
        "\n",
        "    # Load the JSON file if it exists\n",
        "    with open(file_path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# def save_json(dir, mp3_file, data):\n",
        "#     # Replace .mp3 extension with .json\n",
        "#     json_file_name = mp3_file.replace('.mp3', '.json')\n",
        "#     file_path = os.path.join(dir, json_file_name)\n",
        "\n",
        "#     # Check if directory exists, create it if not\n",
        "#     if not os.path.exists(dir):\n",
        "#         os.makedirs(dir)\n",
        "\n",
        "#     # Save the data to the .json file\n",
        "#     with open(file_path, 'w') as file:\n",
        "#         json.dump(data, file, indent=4)\n",
        "\n",
        "def iterate_files(dir):\n",
        "    for file_name in os.listdir(dir):\n",
        "        if file_name.endswith('.mp3'):\n",
        "            yield file_name\n",
        "\n",
        "\n",
        "def calc_features(segment_preds, drums, bass, vocals):\n",
        "  features = []\n",
        "  for audio_class in sound_classes_audioset:\n",
        "    if audio_class == 'drums':\n",
        "      features.append(drums)\n",
        "    elif audio_class == 'bass':\n",
        "      features.append(bass)\n",
        "    elif audio_class == 'vocals':\n",
        "      features.append(vocals)\n",
        "    else:\n",
        "      audioset_indices = [ audio_set_cls['index'] for audio_set_cls in sound_classes_audioset[audio_class]]\n",
        "      features.append(segment_preds[:, audioset_indices].sum(axis=1))\n",
        "\n",
        "  # return np.array(features)\n",
        "  return np.stack(features, axis=1)\n"
      ],
      "metadata": {
        "id": "K2VIt6DosrWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_lengths =  []\n",
        "segment_preds = []\n",
        "drums = []\n",
        "bass = []\n",
        "other = []\n",
        "vocals = []\n",
        "for mp3_file in iterate_files(input_path):\n",
        "  data = load_json(output_path, mp3_file)\n",
        "  if ('demucs' in data.keys() and 'segments' in data.keys()):\n",
        "    segments = data['segments']\n",
        "    for segment in segments:\n",
        "      length = segment['end_idx'] - segment['start_idx']\n",
        "      segment_lengths.append(length)\n",
        "      segment_preds.append(segment['preds'])\n",
        "      drums.append(segment['drums'])\n",
        "      bass.append(segment['bass'])\n",
        "      other.append(segment['other'])\n",
        "      vocals.append(segment['vocals'])\n",
        "\n"
      ],
      "metadata": {
        "id": "IF_UncbNsizZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot frequency chart of segment_lengths\n",
        "plt.hist(segment_lengths, bins=20)\n",
        "plt.xlabel(\"Segment Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Frequency Chart of Segment Lengths\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fwhQOBs7y-pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_preds = np.array(segment_preds)\n",
        "drums = np.array(drums)\n",
        "bass = np.array(bass)\n",
        "other = np.array(other)\n",
        "vocals = np.array(vocals)\n",
        "demucs_features = np.stack((drums, bass, vocals, other), axis=1)"
      ],
      "metadata": {
        "id": "f9Gbo4KQ0Ybp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = calc_features(segment_preds, drums, bass, vocals);\n",
        "features.shape"
      ],
      "metadata": {
        "id": "SeV14J9qI0XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize features\n",
        "normalized_features = (features - np.mean(features, axis=0, keepdims=True)) / np.std(features, axis=0, keepdims=True)"
      ],
      "metadata": {
        "id": "LhCwQDuRM7tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_cluster(data):\n",
        "  eps_range = np.arange(0.1, 10, 0.1)  # Example range of epsilon values\n",
        "  min_samples = data.shape[0] // 100\n",
        "\n",
        "  best_silhouette_score = -1\n",
        "  silhouette_scores = []\n",
        "  nb_labels = []\n",
        "  nb_noise = []\n",
        "  best_eps = None\n",
        "  best_labels = None\n",
        "\n",
        "  for eps in eps_range:\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = dbscan.fit_predict(data)\n",
        "\n",
        "\n",
        "\n",
        "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "    best_label = unique_labels[np.argmax(label_counts)]\n",
        "\n",
        "    # Ignore noisy samples (-1) for silhouette calculation\n",
        "    if len(np.unique(labels)) > 2 and best_label != -1:\n",
        "      data_without_noise = data[labels != -1]\n",
        "      labels_without_noise = labels[labels != -1]\n",
        "\n",
        "      silhouette_avg = silhouette_score(data_without_noise, labels_without_noise, metric='euclidean')\n",
        "      silhouette_scores.append(silhouette_avg)\n",
        "      nb_labels.append(len(np.unique(labels)))\n",
        "      nb_noise.append(np.count_nonzero(labels == -1))\n",
        "\n",
        "      if silhouette_avg > best_silhouette_score:\n",
        "        best_silhouette_score = silhouette_avg\n",
        "        best_eps = eps\n",
        "        best_labels = labels\n",
        "    else:\n",
        "      silhouette_scores.append(-1)\n",
        "      nb_labels.append(-1)\n",
        "      nb_noise.append(-1)\n",
        "\n",
        "  print(f\"Best epsilon: {best_eps}\")\n",
        "  print(f\"Best Silhouette Score: {best_silhouette_score}\")\n",
        "\n",
        "  # Perform DBSCAN clustering with the best parameters\n",
        "  dbscan = DBSCAN(eps=best_eps, min_samples=min_samples)\n",
        "  labels = dbscan.fit_predict(data)\n",
        "\n",
        "  # Print cluster labels for each sample\n",
        "  print(\"Cluster labels for the data points:\", labels)\n",
        "\n",
        "  unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "  best_label = unique_labels[np.argmax(label_counts)]\n",
        "  print(f\"Cluster label with max count: {best_label}\")\n",
        "\n",
        "  # Plot the frequencies of labels\n",
        "  plt.bar(unique_labels, label_counts)\n",
        "  plt.xlabel('Cluster Labels')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.title('Frequency of Cluster Labels')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot Silhouette Score for each k\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.plot(eps_range, silhouette_scores, 'bx-')\n",
        "  plt.xlabel('eps')\n",
        "  plt.ylabel('Silhouette Score')\n",
        "  plt.title('Silhouette Score for Different eps')\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.plot(eps_range, nb_labels, 'bx-')\n",
        "  plt.xlabel('eps')\n",
        "  plt.ylabel('Nb labels')\n",
        "  plt.title('Nb labels for Different eps')\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.plot(eps_range, nb_noise, 'bx-')\n",
        "  plt.xlabel('eps')\n",
        "  plt.ylabel('Nb noise')\n",
        "  plt.title('Nb noise for Different eps')\n",
        "  plt.show()\n",
        "\n",
        "  indexes_best_cluster = np.where(labels == best_label)[0]\n",
        "\n",
        "\n",
        "  return indexes_best_cluster\n",
        "\n"
      ],
      "metadata": {
        "id": "RgJ4lPuy2sHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes_best_cluster = find_best_cluster(normalized_features)\n",
        "features_best_cluster = features[indexes_best_cluster]\n",
        "features_non_best_cluster = features[~np.isin(np.arange(features.shape[0]), indexes_best_cluster)]"
      ],
      "metadata": {
        "id": "5WvMwSc13Irr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.stats import chi2\n"
      ],
      "metadata": {
        "id": "HssFvCouCQ-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_components = 8\n",
        "gmm = GaussianMixture(n_components=n_components, covariance_type='full')\n",
        "gmm.fit(features_best_cluster)\n"
      ],
      "metadata": {
        "id": "ChLpUX1yCT_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = features_best_cluster\n",
        "\n",
        "# Test verschillende aantallen componenten\n",
        "n_components_range = range(1, 11)  # Test van 1 tot 10 componenten\n",
        "aic_scores = []\n",
        "bic_scores = []\n",
        "\n",
        "for n_components in n_components_range:\n",
        "    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n",
        "    gmm.fit(X)\n",
        "\n",
        "    # Voeg AIC en BIC toe aan de lijsten\n",
        "    aic_scores.append(gmm.aic(X))\n",
        "    bic_scores.append(gmm.bic(X))\n",
        "\n",
        "# Plot de AIC en BIC scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(n_components_range, aic_scores, label='AIC', marker='o')\n",
        "plt.plot(n_components_range, bic_scores, label='BIC', marker='o')\n",
        "plt.xlabel('Aantal componenten')\n",
        "plt.ylabel('Score')\n",
        "plt.title('AIC en BIC om het beste aantal componenten te bepalen')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ccV3v9XMGyeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = features_best_cluster\n",
        "\n",
        "# log_likelihood = gmm.score_samples(features_best_cluster)\n",
        "\n",
        "\n",
        "# Bereken de Mahalanobis distance per component\n",
        "mahalanobis_distances = []\n",
        "for i in range(gmm.n_components):\n",
        "    diff = X - gmm.means_[i]\n",
        "    inv_cov = np.linalg.inv(gmm.covariances_[i])\n",
        "    mahalanobis_distances.append(np.sum(np.dot(diff, inv_cov) * diff, axis=1))\n",
        "\n",
        "# Neem de minimale Mahalanobis afstand voor elk punt (dichtstbijzijnde component)\n",
        "mahalanobis_distance = np.min(mahalanobis_distances, axis=0)\n",
        "\n",
        "# Chi2 threshold bij 95% confidence voor het aantal dimensies\n",
        "p_value_threshold = 0.90\n",
        "chi2_threshold = chi2.ppf(p_value_threshold, df=X.shape[1])\n",
        "\n",
        "# Tel de punten binnen en buiten de chi2 threshold\n",
        "nb_in  = np.count_nonzero(mahalanobis_distance <= chi2_threshold)\n",
        "nb_out = np.count_nonzero(mahalanobis_distance > chi2_threshold)\n",
        "\n",
        "print(f\"Aantal punten binnen de 95% drempel: {nb_in}\")\n",
        "print(f\"Aantal punten buiten de 95% drempel: {nb_out}\")\n"
      ],
      "metadata": {
        "id": "A7tcB7uuC_Nd",
        "outputId": "3d1ed385-eff2-4872-c310-7340c25968d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aantal punten binnen de 95% drempel: 195\n",
            "Aantal punten buiten de 95% drempel: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Range of clusters to try (from 2 to 5)\n",
        "cluster_range = range(2, 40)\n",
        "\n",
        "# List to store SSE (sum of squared distances) for the elbow method\n",
        "sse = []\n",
        "silhouette_scores = []\n",
        "\n",
        "\n",
        "# pca = PCA(n_components=20)  # Reduce to 50 dimensions or fewer\n",
        "\n",
        "\n",
        "# combined = np.concatenate((segment_preds, demucs_features * 10 ), axis=1)\n",
        "# data = pca.fit_transform(combined)\n",
        "data = normalized_features\n",
        "\n",
        "# Perform KMeans clustering for different values of k\n",
        "for k in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(data)\n",
        "    sse.append(kmeans.inertia_)  # SSE for elbow method\n",
        "    silhouette_avg = silhouette_score(data, kmeans.labels_)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "# Plot SSE for elbow method\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cluster_range, sse, 'bx-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('SSE (Sum of Squared Distances)')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()\n",
        "\n",
        "# Plot Silhouette Score for each k\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cluster_range, silhouette_scores, 'bx-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score for Different k')\n",
        "plt.show()\n",
        "\n",
        "# Choose the best k based on visual inspection of the elbow and silhouette score\n",
        "best_k = cluster_range[np.argmax(silhouette_scores)]\n",
        "print(f\"Best number of clusters: {best_k}\")\n",
        "\n",
        "# Perform KMeans clustering with the best k\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
        "labels = kmeans.fit_predict(data)\n",
        "\n",
        "# Print cluster labels for each sample\n",
        "print(\"Cluster labels for the data points:\", labels)\n",
        "\n",
        "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "# Plot the frequencies of labels\n",
        "plt.bar(unique_labels, label_counts)\n",
        "plt.xlabel('Cluster Labels')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of Cluster Labels')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x29EZclA0v8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cluster data=normalized_features now with DBSCAN instead of kmeans\n",
        "\n",
        "\n",
        "\n",
        "# Data is already normalized_features\n",
        "\n",
        "# Perform DBSCAN clustering with different values of eps and min_samples\n",
        "eps_range = np.arange(0.1, 10, 0.1)  # Example range of epsilon values\n",
        "min_samples_range = range(3, 5)  # Example range of min_samples values\n",
        "\n",
        "best_silhouette_score = -1\n",
        "best_eps = None\n",
        "best_min_samples = None\n",
        "best_labels = None\n",
        "\n",
        "for eps in eps_range:\n",
        "    for min_samples in min_samples_range:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(data)\n",
        "\n",
        "        # Ignore noisy samples (-1) for silhouette calculation\n",
        "        if len(np.unique(labels)) > 1:\n",
        "            silhouette_avg = silhouette_score(data, labels, metric='euclidean')\n",
        "\n",
        "            if silhouette_avg > best_silhouette_score:\n",
        "                best_silhouette_score = silhouette_avg\n",
        "                best_eps = eps\n",
        "                best_min_samples = min_samples\n",
        "                best_labels = labels\n",
        "\n",
        "print(f\"Best epsilon: {best_eps}\")\n",
        "print(f\"Best min_samples: {best_min_samples}\")\n",
        "print(f\"Best Silhouette Score: {best_silhouette_score}\")\n",
        "\n",
        "# Perform DBSCAN clustering with the best parameters\n",
        "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
        "labels = dbscan.fit_predict(data)\n",
        "\n",
        "# Print cluster labels for each sample\n",
        "print(\"Cluster labels for the data points:\", labels)\n",
        "\n",
        "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "# Plot the frequencies of labels\n",
        "plt.bar(unique_labels, label_counts)\n",
        "plt.xlabel('Cluster Labels')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of Cluster Labels')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Nxhf61UNi4aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for each clustercenter print largest dimensions. Use the dimension_names\n",
        "\n",
        "dimension_names = [audio_class for  audio_class in sound_classes_audioset]\n",
        "\n",
        "for i, cluster_center in enumerate(kmeans.cluster_centers_):\n",
        "  top_indices = np.argsort(cluster_center)[::-1]  # Sort in descending order\n",
        "  print(f\"\\nCluster {i + 1}: Top Dimensions\")\n",
        "  for j in range(5):  # Print top 5 dimensions\n",
        "    dimension_index = top_indices[j]\n",
        "    print(f\"  {dimension_names[dimension_index]}: {cluster_center[dimension_index]:.4f}\")\n"
      ],
      "metadata": {
        "id": "HzjeTt0APt8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the PCA model to a file\n",
        "def save_pca_model(pca, filename=\"pca_model.pkl\"):\n",
        "  with open(filename, \"wb\") as f:\n",
        "    pickle.dump(pca, f)\n",
        "\n",
        "# Load the PCA model from a file\n",
        "def load_pca_model(filename=\"pca_model.pkl\"):\n",
        "  with open(filename, \"rb\") as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "# Project new data using the loaded PCA model\n",
        "def transform_data(pca_model, new_data):\n",
        "  return pca_model.transform(new_data)\n",
        "\n",
        "def save_kmeans_model(kmeans, filename=\"kmeans_model.pkl\"):\n",
        "  with open(filename, \"wb\") as f:\n",
        "    pickle.dump(kmeans, f)\n",
        "\n",
        "# Load the KMeans model from a file\n",
        "def load_kmeans_model(filename=\"kmeans_model.pkl\"):\n",
        "  with open(filename, \"rb\") as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "# Predict cluster labels for new data using the loaded KMeans model\n",
        "def predict_cluster_labels(kmeans_model, new_data):\n",
        "  return kmeans_model.predict(new_data)\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Assuming 'pca' is your trained PCA model\n",
        "save_pca_model(pca)\n",
        "\n",
        "# Assuming 'kmeans' is your trained KMeans model\n",
        "save_kmeans_model(kmeans)\n",
        "\n",
        "# Later, to load and use the model:\n",
        "loaded_pca = load_pca_model()\n",
        "\n",
        "# Later, to load and use the model:\n",
        "loaded_kmeans = load_kmeans_model()\n",
        "\n",
        "# Example new data\n",
        "new_data = np.random.rand(10, 531) # Replace with your actual new data\n",
        "\n",
        "# Project the new data using the loaded PCA model\n",
        "transformed_data = transform_data(loaded_pca, new_data)\n",
        "print(transformed_data.shape)\n",
        "\n",
        "# Predict the cluster labels for the new data\n",
        "new_labels = predict_cluster_labels(loaded_kmeans, transformed_data)\n",
        "print(new_labels)"
      ],
      "metadata": {
        "id": "TGtIo5sx4f7i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}