{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyyNA8SpAtl1MYk3GSB1kN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joris-vaneyghen/mss-jazz-playalong/blob/main/segmentation/cluster_segments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhDdvn1sr6nF"
      },
      "outputs": [],
      "source": [
        "# download our audio example\n",
        "!git clone https://github.com/joris-vaneyghen/mss-jazz-playalong.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "zNHLwMflsUEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = 'mss-jazz-playalong/examples'\n",
        "output_path = 'mss-jazz-playalong/out/segment_and_tag'\n",
        "resolution = 0.32 # resolution of EfficientAT model"
      ],
      "metadata": {
        "id": "afJdx6ogsZcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(dir, mp3_file):\n",
        "    # Replace .mp3 extension with .json\n",
        "    json_file_name = mp3_file.replace('.mp3', '.json')\n",
        "    file_path = os.path.join(dir, json_file_name)\n",
        "\n",
        "    # Check if the .json file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        return {}  # Return an empty dictionary if the .json file doesn't exist\n",
        "\n",
        "    # Load the JSON file if it exists\n",
        "    with open(file_path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# def save_json(dir, mp3_file, data):\n",
        "#     # Replace .mp3 extension with .json\n",
        "#     json_file_name = mp3_file.replace('.mp3', '.json')\n",
        "#     file_path = os.path.join(dir, json_file_name)\n",
        "\n",
        "#     # Check if directory exists, create it if not\n",
        "#     if not os.path.exists(dir):\n",
        "#         os.makedirs(dir)\n",
        "\n",
        "#     # Save the data to the .json file\n",
        "#     with open(file_path, 'w') as file:\n",
        "#         json.dump(data, file, indent=4)\n",
        "\n",
        "def iterate_files(dir):\n",
        "    for file_name in os.listdir(dir):\n",
        "        if file_name.endswith('.mp3'):\n",
        "            yield file_name"
      ],
      "metadata": {
        "id": "K2VIt6DosrWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_lengths =  []\n",
        "segment_preds = []\n",
        "drums = []\n",
        "bass = []\n",
        "other = []\n",
        "vocals = []\n",
        "for mp3_file in iterate_files(input_path):\n",
        "  data = load_json(output_path, mp3_file)\n",
        "  if ('demucs' in data.keys() and 'segments' in data.keys()):\n",
        "    segments = data['segments']\n",
        "    for segment in segments:\n",
        "      length = segment['end_idx'] - segment['start_idx']\n",
        "      segment_lengths.append(length)\n",
        "      segment_preds.append(segment['preds'])\n",
        "      drums.append(segment['drums'])\n",
        "      bass.append(segment['bass'])\n",
        "      other.append(segment['other'])\n",
        "      vocals.append(segment['vocals'])\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "IF_UncbNsizZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot frequency chart of segment_lengths\n",
        "plt.hist(segment_lengths, bins=20)\n",
        "plt.xlabel(\"Segment Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Frequency Chart of Segment Lengths\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fwhQOBs7y-pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_preds = np.array(segment_preds)\n",
        "drums = np.array(drums)\n",
        "bass = np.array(bass)\n",
        "other = np.array(other)\n",
        "vocals = np.array(vocals)\n",
        "demucs_features = np.stack((drums, bass, other, vocals), axis=1)"
      ],
      "metadata": {
        "id": "f9Gbo4KQ0Ybp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Range of clusters to try (from 2 to 5)\n",
        "cluster_range = range(8, 50)\n",
        "\n",
        "# List to store SSE (sum of squared distances) for the elbow method\n",
        "sse = []\n",
        "silhouette_scores = []\n",
        "\n",
        "\n",
        "pca = PCA(n_components=20)  # Reduce to 50 dimensions or fewer\n",
        "\n",
        "\n",
        "combined = np.concatenate((segment_preds, demucs_features * 10 ), axis=1)\n",
        "data = pca.fit_transform(combined)\n",
        "data2 = pca.transform(combined)\n",
        "\n",
        "# Perform KMeans clustering for different values of k\n",
        "for k in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(data)\n",
        "    sse.append(kmeans.inertia_)  # SSE for elbow method\n",
        "    silhouette_avg = silhouette_score(data, kmeans.labels_)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "# Plot SSE for elbow method\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cluster_range, sse, 'bx-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('SSE (Sum of Squared Distances)')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()\n",
        "\n",
        "# Plot Silhouette Score for each k\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cluster_range, silhouette_scores, 'bx-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score for Different k')\n",
        "plt.show()\n",
        "\n",
        "# Choose the best k based on visual inspection of the elbow and silhouette score\n",
        "best_k = cluster_range[np.argmax(silhouette_scores)]\n",
        "print(f\"Best number of clusters: {best_k}\")\n",
        "\n",
        "# Perform KMeans clustering with the best k\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
        "labels = kmeans.fit_predict(data)\n",
        "\n",
        "# Print cluster labels for each sample\n",
        "print(\"Cluster labels for the data points:\", labels)\n",
        "\n",
        "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "# Plot the frequencies of labels\n",
        "plt.bar(unique_labels, label_counts)\n",
        "plt.xlabel('Cluster Labels')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of Cluster Labels')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x29EZclA0v8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot frequencies of labels\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "NPW1QNlO-bT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the PCA model to a file\n",
        "def save_pca_model(pca, filename=\"pca_model.pkl\"):\n",
        "  with open(filename, \"wb\") as f:\n",
        "    pickle.dump(pca, f)\n",
        "\n",
        "# Load the PCA model from a file\n",
        "def load_pca_model(filename=\"pca_model.pkl\"):\n",
        "  with open(filename, \"rb\") as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "# Project new data using the loaded PCA model\n",
        "def transform_data(pca_model, new_data):\n",
        "  return pca_model.transform(new_data)\n",
        "\n",
        "def save_kmeans_model(kmeans, filename=\"kmeans_model.pkl\"):\n",
        "  with open(filename, \"wb\") as f:\n",
        "    pickle.dump(kmeans, f)\n",
        "\n",
        "# Load the KMeans model from a file\n",
        "def load_kmeans_model(filename=\"kmeans_model.pkl\"):\n",
        "  with open(filename, \"rb\") as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "# Predict cluster labels for new data using the loaded KMeans model\n",
        "def predict_cluster_labels(kmeans_model, new_data):\n",
        "  return kmeans_model.predict(new_data)\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Assuming 'pca' is your trained PCA model\n",
        "save_pca_model(pca)\n",
        "\n",
        "# Assuming 'kmeans' is your trained KMeans model\n",
        "save_kmeans_model(kmeans)\n",
        "\n",
        "# Later, to load and use the model:\n",
        "loaded_pca = load_pca_model()\n",
        "\n",
        "# Later, to load and use the model:\n",
        "loaded_kmeans = load_kmeans_model()\n",
        "\n",
        "# Example new data\n",
        "new_data = np.random.rand(10, 531) # Replace with your actual new data\n",
        "\n",
        "# Project the new data using the loaded PCA model\n",
        "transformed_data = transform_data(loaded_pca, new_data)\n",
        "print(transformed_data.shape)\n",
        "\n",
        "# Predict the cluster labels for the new data\n",
        "new_labels = predict_cluster_labels(loaded_kmeans, transformed_data)\n",
        "print(new_labels)"
      ],
      "metadata": {
        "id": "TGtIo5sx4f7i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}