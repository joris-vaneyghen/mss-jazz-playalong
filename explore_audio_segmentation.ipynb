{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqcmKaKQ03IrdfnUFExkBc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joris-vaneyghen/mss-jazz-playalong/blob/main/explore_audio_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Segmentation & Musical Instrument Tagger\n",
        "\n",
        "### Objective:\n",
        "The goal of this project is to segment an audio file into distinct sections and tag each section with the instruments being played.\n",
        "\n",
        "### Requirements:\n",
        "- Each segment should be at least **2,5 seconds** in length.\n",
        "- Consecutive segments should feature **different sets of instruments**."
      ],
      "metadata": {
        "id": "VF5-UYQQ5tbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install demucs -q"
      ],
      "metadata": {
        "id": "-nc7D_hgVxw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ruptures -q"
      ],
      "metadata": {
        "id": "fz1a0Vwgaytl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download our audio example\n",
        "!git clone https://github.com/joris-vaneyghen/mss-jazz-playalong.git"
      ],
      "metadata": {
        "id": "E96R6cKAwG-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dowload our audio tagger\n",
        "!git clone https://github.com/fschmid56/EfficientAT"
      ],
      "metadata": {
        "id": "K3fiDImkzam4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's listen to our audio example\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "Audio('mss-jazz-playalong/examples/Sweet Dreams_Single Ladies.mp3')\n"
      ],
      "metadata": {
        "id": "05PGOOoPzHuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the audio tagger detect the acoustic events in our audio example. This will print the top 10 detected acoustic events (set runtime type to GPU for faster run)\n",
        "#!cd EfficientAT && python inference.py --cuda --model_name=dymn20_as --audio_path=\"../mss-jazz-playalong/examples/Jazz Standards Medley.mp3\""
      ],
      "metadata": {
        "id": "g_9gtJXi0Ssq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instrument Detection Limitations:\n",
        "\n",
        "Our audio tagger successfully detects instruments such as **Singing, Saxophone, Trombone, and Trumpet**, but tends to ignore **drums** and **double bass**. This limitation arises because the tagger was trained on the **Audioset** dataset, which uses **weakly-labeled** data. In this dataset, **drums** and **bass** were often overlooked, leading to reduced detection accuracy for these instruments.\n"
      ],
      "metadata": {
        "id": "0G3puaP53DtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segmentation Approach:\n",
        "\n",
        "To segment the audio, we convert the waveform into a **multi-dimensional time series** of sound class detections using our audio tagger. This results in a time series with **527 dimensions**, each corresponding to one of the sound classes detected by the tagger.\n",
        "\n",
        "For detecting change points in this time series, we use the **Ruptures** library, which is well-suited for this task due to several reasons:\n",
        "\n",
        "- **Versatility**: Ruptures can handle a wide range of data types and is adaptable to different segmentation problems, making it ideal for complex multi-dimensional audio data.\n",
        "- **Efficiency**: It is optimized for large datasets, allowing fast and accurate detection of change points, even when dealing with high-dimensional time series.\n",
        "- **Customizability**: Ruptures offers a variety of methods (e.g., dynamic programming, window-based detection) that can be tailored to our specific needs, ensuring robust and reliable segmentation.\n",
        "\n",
        "By using Ruptures, we can effectively identify moments where the instrument set or sound profile changes, leading to precise audio segmentation.\n"
      ],
      "metadata": {
        "id": "T-hC_hAD_i4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd EfficientAT/"
      ],
      "metadata": {
        "id": "hDoPttxq_jfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspired on EfficientAT/inference.py we load the audio tagger model"
      ],
      "metadata": {
        "id": "NSU3QZu3HzH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.dymn.model import get_model as get_dymn\n",
        "from models.preprocess import AugmentMelSTFT\n",
        "from helpers.utils import NAME_TO_WIDTH\n",
        "\n",
        "\n",
        "def load_mel_and_dymn20_as(device):\n",
        "    \"\"\"\n",
        "    Load the model and mel spectrogram processor for audio tagging.\n",
        "\n",
        "    Args:\n",
        "        device (torch.device): The device to load the model onto (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        mel (AugmentMelSTFT): Mel spectrogram processor.\n",
        "        model (torch.nn.Module): Loaded model.\n",
        "    \"\"\"\n",
        "    sample_rate=32000\n",
        "    window_size=800\n",
        "    hop_size=320\n",
        "    n_mels=128\n",
        "    strides=[2, 2, 2, 2]\n",
        "    model_name = 'dymn20_as'\n",
        "\n",
        "    model = get_dymn(width_mult=NAME_TO_WIDTH(model_name), pretrained_name=model_name, strides=strides)\n",
        "\n",
        "    # Send model to the specified device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Create a mel spectrogram processor (preprocessor)\n",
        "    mel = AugmentMelSTFT(n_mels=n_mels, sr=sample_rate, win_length=window_size, hopsize=hop_size)\n",
        "    mel.to(device)\n",
        "    mel.eval()\n",
        "\n",
        "    return mel, model\n"
      ],
      "metadata": {
        "id": "pTIer-SMDdxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customizing Model Output: Retaining the Time Dimension\n",
        "\n",
        "In our customized model, we aim to keep the **Time** dimension while processing the waveform:\n",
        "\n",
        "1. **Stereo Channels as Batch**:  \n",
        "   In the original implementation, a mono waveform is processed. In our case, we use a stereo waveform and stack the left and right channel at the batch dimension.\n",
        "\n",
        "2. **Time-Frequency Domain Conversion**:  \n",
        "   After the input waveform is converted into the Time-Frequency domain, the model compresses both the **Time** and **Frequency** dimensions by a factor of 32.\n",
        "\n",
        "2. **Pooling Operation Before MLP Layers**:  \n",
        "   Before the final MLP layers, the model performs an **Average Pooling** operation. However, instead of averaging over the Time and Frequency dimensions, we choose to:\n",
        "   - Retain the **Time** dimension.\n",
        "   - Perform the averaging across the **Batch** and **Frequency** dimensions.\n",
        "\n",
        "This approach ensures that the models output can be used as a multi-dimensional time series.\n"
      ],
      "metadata": {
        "id": "J5ncu5wvWn-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from torch import autocast\n",
        "from contextlib import nullcontext\n",
        "\n",
        "def preds_over_time(mel, model, waveform, device):\n",
        "  waveform = torch.from_numpy(waveform).to(device) # shape = (C=2,L)\n",
        "  with torch.no_grad(), autocast(device_type=device) if device == 'cuda' else nullcontext():\n",
        "    spec = mel(waveform) # shape = (C, F=128, T=L/320)\n",
        "    input = spec.unsqueeze(1) # shape = (N=C, D=1, F, T)\n",
        "    # print(input.shape)\n",
        "    features = model._feature_forward(input) # shape = (N, D=1920, F'=F/32, T'â‰ƒT/32)\n",
        "    # print(features.shape)\n",
        "    #We permute Time with Batch dimensions so that pooling is done on the batch and frequency dimension\n",
        "    features = features.permute(3, 1, 2, 0) # shape = (T', F', C', N)\n",
        "    preds, embed = model._clf_forward(features)\n",
        "    preds = torch.sigmoid(preds.float()).squeeze().cpu().numpy() # shape = (T', D'=527)\n",
        "  return preds, embed.cpu().numpy(), features.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "2TY3oYdzZVUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets test this on our example\n",
        "audio_path = '../mss-jazz-playalong/examples/Sweet Dreams_Single Ladies.mp3'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "mel, model = load_mel_and_dymn20_as(device)\n",
        "(waveform, _) = librosa.core.load(audio_path, sr=32000, mono=False)\n",
        "\n",
        "preds, embed, features = preds_over_time(mel, model, waveform, device)"
      ],
      "metadata": {
        "id": "FdPO35CSImA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The time dimension is reduced by a factor 10240 (hop_size * compress_factor)\n",
        "import math\n",
        "assert math.ceil(waveform.shape[1] / (320 * 32)) == preds.shape[0]"
      ],
      "metadata": {
        "id": "uH1mPFDPXMNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(preds[:, 27])\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Probability of Singing\")\n",
        "plt.title(\"Probability of Singing over Time\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(preds[:, 153])\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Probability of Piano\")\n",
        "plt.title(\"Probability of Piano over Time\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(preds[:, 197])\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Probability of Sax\")\n",
        "plt.title(\"Probability of Sax over Time\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ou6UUx03ajP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ruptures as rpt\n",
        "\n",
        "# signal = preds[:, [27, 153, 197]]\n",
        "signal = embed\n",
        "\n",
        "# Stel de minimale lengte van een segment in op 8 (= 2,56 seconden. namelijk 8 * (32 * 320) / 32000)\n",
        "min_size = 8\n",
        "\n",
        "# Gebruik de Pelt-methode voor breekpuntdetectie\n",
        "model = \"rbf\"  # Verandering in gemiddelde (kan aangepast worden naar andere methoden zoals \"l1\", \"l2\", \"rbf\")\n",
        "algo = rpt.Pelt(model=model, min_size=min_size, jump=1).fit(signal)\n",
        "\n",
        "\n",
        "# Detecteer breekpunten, zonder het aantal vooraf te specificeren\n",
        "penalty = 4  # Penalty bepaalt hoe streng we breekpunten toestaan, je kunt hiermee spelen\n",
        "bkps = algo.predict(pen=penalty)\n",
        "\n",
        "# Plot het resultaat\n",
        "rpt.display(preds[:, 153], bkps, figsize=(10, 6))  # Plot alleen de piano dimensie\n",
        "plt.title(\"Detectie van breekpunten in het multidimensionale signaal\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Print de gevonden breekpunten\n",
        "print(\"# breekpunten:\", len(bkps))\n",
        "print(\"Gevonden breekpunten:\", bkps)\n"
      ],
      "metadata": {
        "id": "SuJav7jTbDKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(waveform, sr) = librosa.core.load(audio_path, mono=True)\n",
        "f = (10240 * sr) // 32000"
      ],
      "metadata": {
        "id": "NealS0JNUK_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Audio(waveform[0: bkps[0] * f], rate=sr))\n",
        "for i in range(len(bkps)-1):\n",
        "  display(Audio(waveform[bkps[i] * f: bkps[i+1] * f], rate=sr))"
      ],
      "metadata": {
        "id": "6GL-uXdAUwhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!demucs -n htdemucs_ft \"../mss-jazz-playalong/examples/Sweet Dreams_Single Ladies.mp3\" -o out"
      ],
      "metadata": {
        "id": "nZ9bvnoeYA_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(\"out/htdemucs_ft/Sweet Dreams_Single Ladies/bass.wav\")"
      ],
      "metadata": {
        "id": "Q7CpTK4tfndY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: given numpy.ndarray waveform, caculate an average of absolute values per block of 10240\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def calculate_average_db_per_block(waveform, block_size=10240):\n",
        "    \"\"\"\n",
        "    Calculates the average decibels per block of a given size in a waveform.\n",
        "\n",
        "    Args:\n",
        "      waveform: A numpy.ndarray representing the waveform.\n",
        "      block_size: The size of the block for calculating the average.\n",
        "\n",
        "    Returns:\n",
        "      A list of average decibel values for each block.\n",
        "    \"\"\"\n",
        "    averages = []\n",
        "    # Adding a small value to avoid log of zero\n",
        "    epsilon = 1e-10  # Prevent log(0)\n",
        "\n",
        "    for i in range(0, len(waveform), block_size):\n",
        "        block = waveform[i:i + block_size]\n",
        "        if len(block) > 0:\n",
        "            # Calculate absolute value, add epsilon to avoid log(0), then convert to decibels\n",
        "            block_abs = np.abs(block) + epsilon\n",
        "            block_db = 20 * np.log10(block_abs)\n",
        "            # Calculate the average decibels for the block\n",
        "            # average_db = np.mean(block_db)\n",
        "            average_db = np.mean(np.abs(block))\n",
        "            averages.append(average_db)\n",
        "\n",
        "    return np.array(averages)\n",
        "\n",
        "\n",
        "# Example usage (assuming you have the 'waveform' variable defined):\n",
        "# averages_per_block = calculate_average_abs_per_block(waveform)\n",
        "# print(averages_per_block)\n"
      ],
      "metadata": {
        "id": "8U4d4Sjujmza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(bass, sr) = librosa.core.load(\"out/htdemucs_ft/Sweet Dreams_Single Ladies/bass.wav\", mono=True)\n",
        "(drums, sr) = librosa.core.load(\"out/htdemucs_ft/Sweet Dreams_Single Ladies/drums.wav\", mono=True)\n",
        "(other, sr) = librosa.core.load(\"out/htdemucs_ft/Sweet Dreams_Single Ladies/other.wav\", mono=True)\n",
        "(vocals, sr) = librosa.core.load(\"out/htdemucs_ft/Sweet Dreams_Single Ladies/vocals.wav\", mono=True)\n",
        "\n",
        "bass = calculate_average_db_per_block(bass, block_size = 10240 * sr//32000)\n",
        "other = calculate_average_db_per_block(other, block_size = 10240 * sr//32000)\n",
        "drums = calculate_average_db_per_block(drums, block_size = 10240 * sr//32000)\n",
        "vocals = calculate_average_db_per_block(vocals, block_size = 10240 * sr//32000)\n",
        "stacked_signal = np.stack((drums, bass, vocals, other), axis=1)\n"
      ],
      "metadata": {
        "id": "P53-vVERkU4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: stack stacked_signal together with embed\n",
        "\n",
        "import numpy as np\n",
        "combined_signal = np.concatenate((stacked_signal, preds[:, [187]]), axis=1)\n",
        "combined_signal.shape"
      ],
      "metadata": {
        "id": "OJwpE1Gss3jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(bass)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Volume of bass\")\n",
        "plt.title(\"Volume of bass over Time\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(other)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Volume of other\")\n",
        "plt.title(\"Volume of other over Time\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(drums)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Volume of drums\")\n",
        "plt.title(\"Volume of drums over Time\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(vocals)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Volume of vocals\")\n",
        "plt.title(\"Volume of vocals over Time\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FsX4g8lmkame"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ruptures as rpt\n",
        "\n",
        "\n",
        "# Stel de minimale lengte van een segment in op 8 (= 2,56 seconden. namelijk 8 * (32 * 320) / 32000)\n",
        "min_size = 8\n",
        "\n",
        "# Gebruik de Pelt-methode voor breekpuntdetectie\n",
        "model = \"normal\"  # Verandering in gemiddelde (kan aangepast worden naar andere methoden zoals \"l1\", \"l2\", \"rbf\")\n",
        "algo = rpt.Pelt(model=model, min_size=min_size, jump=1).fit(stacked_signal)\n",
        "# algo = rpt.Dynp(model=model, min_size=min_size, jump=1).fit(combined_signal)\n",
        "# algo = rpt.Window(width=min_size, model=model).fit(stacked_signal)\n",
        "\n",
        "# Detecteer breekpunten, zonder het aantal vooraf te specificeren\n",
        "penalty = 100  # Penalty bepaalt hoe streng we breekpunten toestaan, je kunt hiermee spelen\n",
        "bkps = algo.predict(pen=penalty)\n",
        "\n",
        "# Plot het resultaat\n",
        "rpt.display(stacked_signal[:, 0], bkps, figsize=(10, 6))  # Plot alleen de other dimensie\n",
        "plt.title(\"Detectie van breekpunten in het multidimensionale signaal\")\n",
        "plt.show()\n",
        "\n",
        "# Plot het resultaat\n",
        "rpt.display(stacked_signal[:, 1], bkps, figsize=(10, 6))  # Plot alleen de other dimensie\n",
        "plt.title(\"Detectie van breekpunten in het multidimensionale signaal\")\n",
        "plt.show()\n",
        "\n",
        "# Plot het resultaat\n",
        "rpt.display(stacked_signal[:, 2], bkps, figsize=(10, 6))  # Plot alleen de other dimensie\n",
        "plt.title(\"Detectie van breekpunten in het multidimensionale signaal\")\n",
        "plt.show()\n",
        "\n",
        "# Plot het resultaat\n",
        "rpt.display(stacked_signal[:, 3], bkps, figsize=(10, 6))  # Plot alleen de other dimensie\n",
        "plt.title(\"Detectie van breekpunten in het multidimensionale signaal\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Print de gevonden breekpunten\n",
        "print(\"# breekpunten:\", len(bkps))\n",
        "print(\"Gevonden breekpunten:\", bkps)\n"
      ],
      "metadata": {
        "id": "eCrYHrotlyhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(waveform, sr) = librosa.core.load(audio_path, mono=True)\n",
        "f = (10240 * sr) // 32000\n",
        "\n",
        "display(Audio(waveform[0: bkps[0] * f], rate=sr))\n",
        "for i in range(len(bkps)-1):\n",
        "  display(Audio(waveform[bkps[i] * f: bkps[i+1] * f], rate=sr))"
      ],
      "metadata": {
        "id": "PBMSVO5Dmtc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=20)  # Reduce to 20 dimensions\n",
        "reduced_data = pca.fit_transform(preds)"
      ],
      "metadata": {
        "id": "5XgQk5lbs7dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = 0\n",
        "new_bkps = []\n",
        "parent = []\n",
        "for i in range(len(bkps)):\n",
        "  end = bkps[i]\n",
        "  signal = reduced_data[start:end]\n",
        "  # Stel de minimale lengte van een segment in op 8 (= 2,56 seconden. namelijk 8 * (32 * 320) / 32000)\n",
        "  min_size = 8\n",
        "  # Gebruik de Pelt-methode voor breekpuntdetectie\n",
        "  model = \"rbf\"  # Verandering in gemiddelde (kan aangepast worden naar andere methoden zoals \"l1\", \"l2\", \"rbf\")\n",
        "  algo = rpt.Pelt(model=model, min_size=min_size, jump=1).fit(signal)\n",
        "  # Detecteer breekpunten, zonder het aantal vooraf te specificeren\n",
        "  penalty = 3  # Penalty bepaalt hoe streng we breekpunten toestaan, je kunt hiermee spelen\n",
        "  sub_bkps = algo.predict(pen=penalty)\n",
        "  parent.extend([i for bkp in sub_bkps])\n",
        "  new_bkps.extend([bkp + start for bkp in sub_bkps])\n",
        "  start  = end\n",
        "\n",
        "\n",
        "print(bkps)\n",
        "print(new_bkps)\n",
        "print(parent)"
      ],
      "metadata": {
        "id": "xI6t2itMpGeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Audio(waveform[0: new_bkps[0] * f], rate=sr))\n",
        "for i in range(len(new_bkps)-1):\n",
        "  print(parent[i + 1])\n",
        "  print(new_bkps[i])\n",
        "  display(Audio(waveform[new_bkps[i] * f: new_bkps[i+1] * f], rate=sr))"
      ],
      "metadata": {
        "id": "mZR0rukyLgln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def features_to_preds(model, features, device):\n",
        "  features = torch.from_numpy(features).to(device)\n",
        "  features = features.permute(3, 1, 2, 0)\n",
        "  features = torch.mean(features, dim=0, keepdim=True)\n",
        "  with torch.no_grad(), autocast(device_type=device) if device == 'cuda' else nullcontext():\n",
        "    preds, embed = model._clf_forward(features)\n",
        "    preds = torch.sigmoid(preds.float()).squeeze().cpu().numpy() # shape = (T', D'=527)\n",
        "  return preds, embed.squeeze().cpu().numpy()"
      ],
      "metadata": {
        "id": "nb-WUK0Jy1AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel, model = load_mel_and_dymn20_as(device)\n"
      ],
      "metadata": {
        "id": "jzyNSIiNzWgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = 0\n",
        "preds_per_segm = []\n",
        "embed_per_segm = []\n",
        "demucs_per_segm = []\n",
        "for i in range(len(new_bkps)):\n",
        "  end = new_bkps[i]\n",
        "  preds_i, embed_i = features_to_preds(model, features[start: end], device);\n",
        "  preds_per_segm.append(preds_i)\n",
        "  embed_per_segm.append(embed_i)\n",
        "  demucs_per_segm.append(stacked_signal[start: end].mean(axis=0))\n",
        "  start = end\n",
        "\n",
        "preds_per_segm = np.vstack(preds_per_segm)\n",
        "embed_per_segm = np.vstack(embed_per_segm)\n",
        "demucs_per_segm= np.vstack(demucs_per_segm)\n",
        "\n",
        "demucs_per_segm.shape"
      ],
      "metadata": {
        "id": "CBZa1QNK32WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NYd2vhE97h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_per_segm[:,194]"
      ],
      "metadata": {
        "id": "RjC4HgfI2cbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Range of clusters to try (from 2 to 5)\n",
        "cluster_range = range(2, 10)\n",
        "\n",
        "# List to store SSE (sum of squared distances) for the elbow method\n",
        "sse = []\n",
        "silhouette_scores = []\n",
        "\n",
        "pca = PCA(n_components=6)  # Reduce to 50 dimensions or fewer\n",
        "data = pca.fit_transform(embed_per_segm)\n",
        "data = np.concatenate((preds_per_segm, demucs_per_segm), axis=1)\n",
        "\n",
        "# Perform KMeans clustering for different values of k\n",
        "for k in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(data)\n",
        "    sse.append(kmeans.inertia_)  # SSE for elbow method\n",
        "    silhouette_avg = silhouette_score(data, kmeans.labels_)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "# Plot SSE for elbow method\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cluster_range, sse, 'bx-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('SSE (Sum of Squared Distances)')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()\n",
        "\n",
        "# Plot Silhouette Score for each k\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cluster_range, silhouette_scores, 'bx-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score for Different k')\n",
        "plt.show()\n",
        "\n",
        "# Choose the best k based on visual inspection of the elbow and silhouette score\n",
        "best_k = cluster_range[np.argmax(silhouette_scores)]\n",
        "print(f\"Best number of clusters: {best_k}\")\n",
        "\n",
        "# Perform KMeans clustering with the best k\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
        "labels = kmeans.fit_predict(data)\n",
        "\n",
        "# Print cluster labels for each sample\n",
        "print(\"Cluster labels for the data points:\", labels)\n"
      ],
      "metadata": {
        "id": "LGKXZqHh5QQE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}