{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI8OlR2/P5rAoEq69M05DT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joris-vaneyghen/mss-jazz-playalong/blob/main/explore_audio_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Segmentation & Musical Instrument Tagger\n",
        "\n",
        "### Objective:\n",
        "The goal of this project is to segment an audio file into distinct sections and tag each section with the instruments being played.\n",
        "\n",
        "### Requirements:\n",
        "- Each segment should be at least **2,5 seconds** in length.\n",
        "- Consecutive segments should feature **different sets of instruments**."
      ],
      "metadata": {
        "id": "VF5-UYQQ5tbl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-nc7D_hgVxw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ruptures"
      ],
      "metadata": {
        "id": "fz1a0Vwgaytl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download our audio example\n",
        "!git clone https://github.com/joris-vaneyghen/mss-jazz-playalong.git"
      ],
      "metadata": {
        "id": "E96R6cKAwG-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dowload our audio tagger\n",
        "!git clone https://github.com/fschmid56/EfficientAT"
      ],
      "metadata": {
        "id": "K3fiDImkzam4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's listen to our audio example\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "Audio('mss-jazz-playalong/examples/Jazz Standards Medley.mp3')\n"
      ],
      "metadata": {
        "id": "05PGOOoPzHuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the audio tagger detect the acoustic events in our audio example. This will print the top 10 detected acoustic events (set runtime type to GPU for faster run)\n",
        "#!cd EfficientAT && python inference.py --cuda --model_name=dymn20_as --audio_path=\"../mss-jazz-playalong/examples/Jazz Standards Medley.mp3\""
      ],
      "metadata": {
        "id": "g_9gtJXi0Ssq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instrument Detection Limitations:\n",
        "\n",
        "Our audio tagger successfully detects instruments such as **Singing, Saxophone, Trombone, and Trumpet**, but tends to ignore **drums** and **double bass**. This limitation arises because the tagger was trained on the **Audioset** dataset, which uses **weakly-labeled** data. In this dataset, **drums** and **bass** were often overlooked, leading to reduced detection accuracy for these instruments.\n"
      ],
      "metadata": {
        "id": "0G3puaP53DtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segmentation Approach:\n",
        "\n",
        "To segment the audio, we convert the waveform into a **multi-dimensional time series** of sound class detections using our audio tagger. This results in a time series with **527 dimensions**, each corresponding to one of the sound classes detected by the tagger.\n",
        "\n",
        "For detecting change points in this time series, we use the **Ruptures** library, which is well-suited for this task due to several reasons:\n",
        "\n",
        "- **Versatility**: Ruptures can handle a wide range of data types and is adaptable to different segmentation problems, making it ideal for complex multi-dimensional audio data.\n",
        "- **Efficiency**: It is optimized for large datasets, allowing fast and accurate detection of change points, even when dealing with high-dimensional time series.\n",
        "- **Customizability**: Ruptures offers a variety of methods (e.g., dynamic programming, window-based detection) that can be tailored to our specific needs, ensuring robust and reliable segmentation.\n",
        "\n",
        "By using Ruptures, we can effectively identify moments where the instrument set or sound profile changes, leading to precise audio segmentation.\n"
      ],
      "metadata": {
        "id": "T-hC_hAD_i4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd EfficientAT/"
      ],
      "metadata": {
        "id": "hDoPttxq_jfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspired on EfficientAT/inference.py we load the audio tagger model"
      ],
      "metadata": {
        "id": "NSU3QZu3HzH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.dymn.model import get_model as get_dymn\n",
        "from models.preprocess import AugmentMelSTFT\n",
        "from helpers.utils import NAME_TO_WIDTH\n",
        "\n",
        "\n",
        "def load_mel_and_dymn20_as(device):\n",
        "    \"\"\"\n",
        "    Load the model and mel spectrogram processor for audio tagging.\n",
        "\n",
        "    Args:\n",
        "        device (torch.device): The device to load the model onto (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        mel (AugmentMelSTFT): Mel spectrogram processor.\n",
        "        model (torch.nn.Module): Loaded model.\n",
        "    \"\"\"\n",
        "    sample_rate=32000\n",
        "    window_size=800\n",
        "    hop_size=320\n",
        "    n_mels=128\n",
        "    strides=[2, 2, 2, 2]\n",
        "    model_name = 'dymn20_as'\n",
        "\n",
        "    model = get_dymn(width_mult=NAME_TO_WIDTH(model_name), pretrained_name=model_name, strides=strides)\n",
        "\n",
        "    # Send model to the specified device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Create a mel spectrogram processor (preprocessor)\n",
        "    mel = AugmentMelSTFT(n_mels=n_mels, sr=sample_rate, win_length=window_size, hopsize=hop_size)\n",
        "    mel.to(device)\n",
        "    mel.eval()\n",
        "\n",
        "    return mel, model\n"
      ],
      "metadata": {
        "id": "pTIer-SMDdxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customizing Model Output: Retaining the Time Dimension\n",
        "\n",
        "In our customized model, we aim to keep the **Time** dimension while processing the waveform:\n",
        "\n",
        "1. **Stereo Channels as Batch**:  \n",
        "   In the original implementation, a mono waveform is processed. In our case, we use a stereo waveform and stack the left and right channel at the batch dimension.\n",
        "\n",
        "2. **Time-Frequency Domain Conversion**:  \n",
        "   After the input waveform is converted into the Time-Frequency domain, the model compresses both the **Time** and **Frequency** dimensions by a factor of 32.\n",
        "\n",
        "2. **Pooling Operation Before MLP Layers**:  \n",
        "   Before the final MLP layers, the model performs an **Average Pooling** operation. However, instead of averaging over the Time and Frequency dimensions, we choose to:\n",
        "   - Retain the **Time** dimension.\n",
        "   - Perform the averaging across the **Batch** and **Frequency** dimensions.\n",
        "\n",
        "This approach ensures that the models output can be used as a multi-dimensional time series.\n"
      ],
      "metadata": {
        "id": "J5ncu5wvWn-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from torch import autocast\n",
        "from contextlib import nullcontext\n",
        "\n",
        "def preds_over_time(mel, model, waveform, device):\n",
        "  waveform = torch.from_numpy(waveform).to(device) # shape = (C=2,L)\n",
        "  with torch.no_grad(), autocast(device_type=device.type) if device == 'cuda' else nullcontext():\n",
        "    spec = mel(waveform) # shape = (C, F=128, T=L/320)\n",
        "    input = spec.unsqueeze(1) # shape = (N=C, D=1, F, T)\n",
        "    print(input.shape)\n",
        "    features = model._feature_forward(input) # shape = (N, D=1920, F'=F/32, T'â‰ƒT/32)\n",
        "    print(features.shape)\n",
        "    #We permute Time with Batch dimensions so that pooling is done on the batch and frequency dimension\n",
        "    features = features.permute(3, 1, 2, 0) # shape = (T', F', C', N)\n",
        "    preds, embed = model._clf_forward(features)\n",
        "    preds = torch.sigmoid(preds.float()).squeeze().cpu().numpy() # shape = (T', D'=527)\n",
        "  return preds, embed\n"
      ],
      "metadata": {
        "id": "2TY3oYdzZVUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets test this on our example\n",
        "audio_path = '../mss-jazz-playalong/examples/Jazz Standards Medley.mp3'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "mel, model = load_mel_and_dymn20_as(device)\n",
        "(waveform, _) = librosa.core.load(audio_path, sr=32000, mono=False)\n",
        "\n",
        "preds, embed = preds_over_time(mel, model, waveform, device)"
      ],
      "metadata": {
        "id": "FdPO35CSImA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The time dimension is reduced by a factor 10240 (hop_size * compress_factor)\n",
        "import math\n",
        "assert math.ceil(waveform.shape[1] / (320 * 32)) == preds.shape[0]"
      ],
      "metadata": {
        "id": "uH1mPFDPXMNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(preds[:, 27])\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Probability of Singing\")\n",
        "plt.title(\"Probability of Singing over Time\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(preds[:, 153])\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Probability of Piano\")\n",
        "plt.title(\"Probability of Piano over Time\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(preds[:, 197])\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Probability of Sax\")\n",
        "plt.title(\"Probability of Sax over Time\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ou6UUx03ajP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ruptures as rpt\n",
        "\n",
        "# signal = preds[:, [27, 153, 197]]\n",
        "signal = embed\n",
        "\n",
        "# Stel de minimale lengte van een segment in op 8 (= 2,56 seconden. namelijk 8 * (32 * 320) / 32000)\n",
        "min_size = 8\n",
        "\n",
        "# Gebruik de Pelt-methode voor breekpuntdetectie\n",
        "model = \"rbf\"  # Verandering in gemiddelde (kan aangepast worden naar andere methoden zoals \"l1\", \"l2\", \"rbf\")\n",
        "algo = rpt.Pelt(model=model, min_size=min_size, jump=1).fit(signal)\n",
        "\n",
        "\n",
        "# Detecteer breekpunten, zonder het aantal vooraf te specificeren\n",
        "penalty = 2.5  # Penalty bepaalt hoe streng we breekpunten toestaan, je kunt hiermee spelen\n",
        "bkps = algo.predict(pen=penalty)\n",
        "\n",
        "# Plot het resultaat\n",
        "rpt.display(preds[:, 153], bkps, figsize=(10, 6))  # Plot alleen de eerste dimensie\n",
        "plt.title(\"Detectie van breekpunten in het multidimensionale signaal\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Print de gevonden breekpunten\n",
        "print(\"# breekpunten:\", len(bkps))\n",
        "print(\"Gevonden breekpunten:\", bkps)\n"
      ],
      "metadata": {
        "id": "SuJav7jTbDKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(waveform, sr) = librosa.core.load(audio_path, mono=True)\n",
        "f = (10240 * sr) // 32000"
      ],
      "metadata": {
        "id": "NealS0JNUK_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Audio(waveform[0: bkps[0] * f], rate=sr))\n",
        "for i in range(len(bkps)-1):\n",
        "  display(Audio(waveform[bkps[i] * f: bkps[i+1] * f], rate=sr))"
      ],
      "metadata": {
        "id": "6GL-uXdAUwhB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}